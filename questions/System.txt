Overview
Chapters
Reviews
FAQ’s
7
Chapters
1
Quiz
10
Articles
9
Case Studies
Discover a smoother learning journey through our  effortless roadmap

9 articles
What is Systems Design - Learn System Design
Functional vs Non Functional Requirements
What are the components of System Design?
Analysis of Monolithic and Distributed Systems - Learn System Design
What are Microservices?
What is Scalability and How to achieve it?
How does the process of developing a system differ from designing a system?
What is High Level Design – Learn System Design
What is Low Level Design or LLD - Learn System Design

3 articles
Design Media Sharing Social Networking System
System Design of Youtube - A Complete Architecture
Designing Twitter - A System Design Interview Question

1 articles
Designing Facebook Messenger | System Design Interview

2 articles
System Design Netflix | A Complete Architecture
Design Dropbox - A System Design Interview Question

1 articles
System Design of Uber App | Uber System Architecture

1 articles
Vending Machine: High Level System Design

1 articles
Low Level Design of Tic Tac Toe | System Design

1 articles
Top MCQs on System Design with Answers
In this course, we have made the complex world of system design easy to understand. Whether you're a newbie or a seasoned pro, this course will equip you with the skills to create efficient and scalable systems. Think of it as a roadmap, guiding you through the process of building and maintaining systems that can handle big challenges. We'll teach you how to break down complex problems into smaller, manageable parts, just like taking a big puzzle and turning it into smaller, solvable pieces.
Learning system design is crucial in the fast-paced world of technology. It's the key to creating efficient, reliable, and scalable solutions, whether you're building software, designing websites, or crafting complex systems. System design empowers you to make the most of resources, ensuring technology runs efficiently and smoothly. It's also a skill highly valued by employers, as it's essential for solving real-world problems and staying competitive in the ever-evolving tech industry. Whether you're a beginner or an experienced professional, grasping system design principles will open doors to better career opportunities and help you excel in a tech-driven world.
Asha Bisht

                                                The organized and complete System Design Guide on GFG is excellent. It has been really incredibly beneficial for me.                                            
Aditya Sharma

                                                This Guide helped me to get placed, each and every concept of this Guide is so well described that a newbie of System Design can grasp easily.                                            
Sachin Motwani

                                                The topics in this Guide are explained clearly, and design problems given were very helpful. Kudos to GeeksforGeeks!!!                                            
Askand Shahi

                                                I have upgraded my technical skills for development. Not only that with the help of this guide, but I was also able to get more job offers from IT companies.                                            
How can I start this course?
You just need to click on the button that says START YOUR JOURNEY, and that's it. You will be taken to your first chapter.
Is this course for newbies?
Whether you're a newbie or a seasoned pro, this course will equip you with the skills to create efficient and scalable systems. Think of it as a roadmap, guiding you through the process of building and maintaining systems that can handle big challenges.

We all know that System Design is the core concept behind the design of any distributed system. Therefore every person in the tech industry needs to have at least a basic understanding of what goes behind designing a System. With this intent, we have brought to you the ultimate System Design Interview Bootcamp, a one-stop solution for learning System Design.

The most important stage in any development process, be it Software or any other tech, is Design. Without the designing phase, you cannot jump to the implementation of the testing part. The same is the case with the System as well.
Important Topics For The System Design Interview Bootcamp
Let us first begin the System Design Interview Bootcamp with the basics and fundamental terms and concepts used in System Design.
Functional Requirements 
These are the requirements that the end user specifically demands as basic facilities that the system should offer. All these functionalities need to be necessarily incorporated into the system as a part of the contract. These are represented or stated in the form of input to be given to the system, the operation performed and the output expected. They are basically the requirements stated by the user which one can see directly in the final product
Non-functional requirements 
These are basically the quality constraints that the system must satisfy according to the project contract. The priority or extent to which these factors are implemented varies from one project to other. They are also called non-behavioral requirements.
Note: Functional vs non-Functional requirements

In horizontal scaling, we enhance the performance of the server by adding more machines to the network and sharing the processing and memory workload across multiple devices. We add more instances of the server to the existing pool of servers and distribute the load among these servers. In this approach, there is no need to change the server’s capacity or replace the server. Also, like vertical scaling, there is no downtime while adding more servers to the network.
In simple terms upgrading the capacity of a single machine or moving to a new machine with more power is called vertical scaling. You can add more power to your machine by adding better processors, increasing RAM, or other power-increasing adjustments. Vertical scaling can be easily achieved by switching from small to bigger machines but remember that this involves downtime.

The three letters in CAP refer to three desirable properties of distributed systems with replicated data: consistency (among replicated copies), availability (of the system for read and write operations) and partition tolerance (in the face of the nodes in the system being partitioned by a network fault). 
The CAP theorem states that it is not possible to guarantee all three of the desirable properties – consistency, availability, and partition tolerance at the same time in a distributed system with data replication. 




Microservice is a small, loosely coupled distributed service. It has evolved as a solution to the scalability, independently deployable, and innovation challenges with Monolithic architecture. It allows you to take a large application and decompose or break it into easily manageable small components with narrowly defined responsibilities. It is considered the building block of modern applications. Microservices can be written in a variety of programming languages, and frameworks, and each service acts as a mini-application on its own. 

Proxy servers act as intermediaries between client devices and servers. They improve performance by caching frequently requested content, provide security by filtering incoming traffic, and enable load balancing for efficient distribution of requests.   

Redundancy in system design is the intentional inclusion of extra components, systems, or resources to ensure continued functionality in the event of a failure. Redundancy aims to eliminate single points of failure, enhancing system reliability and fault tolerance. This can be achieved through techniques such as duplicating critical hardware components or having backup systems ready to take over seamlessly. 


Replication, on the other hand, involves creating and maintaining copies of data or entire systems across multiple locations. The primary goal of replication is to improve data availability, distribute the load, and enhance fault tolerance. In distributed databases, data replication ensures that if one server or node fails, another can take over with the same dataset. This redundancy of data contributes to resilience, reducing the risk of data loss and improving overall system performance. 
System design is the process of designing the architecture and components of a software system to meet specific business requirements. The process involves defining the system’s architecture, components, modules, and interfaces, and identifying the technologies and tools that will be used to implement the system. Here are some steps to get started with system design:
High-level design or HLD refers to the overall system, a design that consists description of the system architecture and design and is a generic system design that includes:
High-level design or HLD is also known as macro level designing.


Block storage involves dividing data into fixed-sized blocks and storing them on block devices such as hard drives or solid-state drives (SSDs). These blocks are accessed using low-level block-level protocols, typically through storage area networks (SANs) or direct-attached storage (DAS).
File Storage is very important in every System Design. So it is very important in this System Design Interview Bootcamp to learn about it in detail.

File storage stores data as files and presents it to its final users as a hierarchical directories structure. It is typically accessed using file-level protocols like Network File System (NFS) or Server Message Block (SMB). File storage can be implemented using network-attached storage (NAS) devices or distributed file systems.


Files are divided into little parts and dispersed over hardware in a flat structure. Instead of being maintained as files in directories or as blocks on servers, the data is divided up into discrete parts called objects and kept in a single repository with object storage. Object storage systems typically use a RESTful API for accessing and managing data.

RAID combines multiple physical disk drives into a single logical unit to improve performance, reliability, or a combination of both. RAID is very transparent to the underlying system. This means, that to the host system, it appears as a single big disk presenting itself as a linear array of blocks. This allows older technologies to be replaced by RAID without making too many changes to the existing code. 
Message queues facilitate communication between distributed systems by allowing asynchronous communication. This decouples the components, enabling them to operate independently and improving system reliability. It is a form of communication and data transfer mechanism used in computer science and system design. It functions as a temporary storage and routing system for messages exchanged between different components, applications, or systems within a larger software architecture.
Kafka is a distributed streaming platform that excels in handling real-time data streams. It is used for building real-time data pipelines and streaming applications.
Google Inc. developed the Google File System (GFS), a scalable distributed file system (DFS), to meet the company’s growing data processing needs. GFS offers fault tolerance, dependability, scalability, availability, and performance to big networks and connected nodes. GFS is made up of a number of storage systems constructed from inexpensive commodity hardware parts. The search engine, which creates enormous volumes of data that must be kept, is only one example of how it is customized to meet Google’s various data use and storage requirements.
With growing data velocity the data size easily outgrows the storage limit of a machine. A solution would be to store the data across a network of machines. Such filesystems are called distributed filesystems. Since data is stored across a network all the complications of a network come in. 
This is where Hadoop comes in. It provides one of the most reliable filesystems. HDFS (Hadoop Distributed File System) is a unique design that provides storage for extremely large files with streaming data access pattern and it runs on commodity hardware. 
Design patterns are used to represent some of the best practices adapted by experienced object-oriented software developers. A design pattern systematically names, motivates, and explains a general design that addresses a recurring design problem in object-oriented systems. It describes the problem, the solution, when to apply the solution, and its consequences. 


It is a space-efficient probabilistic data structure used to test whether a given element is a member of a set. It achieves this by using multiple hash functions to map elements to a bit array. While false positives are possible, false negatives are not. This makes Bloom filters valuable in scenarios where memory is constrained, and a slight risk of false positives is acceptable, such as in caching systems and network routing tables.

It is a technique used in distributed systems to efficiently distribute data across a changing set of nodes. Unlike traditional hash functions, consistent hashing minimizes the impact of adding or removing nodes, ensuring that most keys remain mapped to the same nodes. This is particularly useful in scenarios like distributed caching and load balancing, where maintaining a stable mapping despite node changes is essential for performance and data integrity.

In distributed systems, a quorum is a strategy to achieve consensus among a majority of nodes. It helps in ensuring that a certain number of nodes must agree on an operation for it to be considered successful. Quorums are crucial for maintaining data consistency and availability, especially in scenarios prone to network partitions. Variations like the “two-thirds” or “majority” quorum systems are common in databases, providing a balance between fault tolerance and system responsiveness.

A checksum is a value derived from the data in a file or message, used to verify its integrity. Various algorithms, like CRC or Adler-32, calculate checksums, and if the checksum of the received data matches the calculated checksum, it indicates that the data is likely intact. Checksums are widely used in data transmission and storage to detect errors, ensuring data reliability and preventing the propagation of corrupt information.

It is a hierarchical data structure that facilitates efficient verification of large datasets. It works by recursively hashing pairs of data until a single hash, known as the Merkle root, is obtained. If any part of the data changes, it only affects the path from the altered data to the root, simplifying verification. Merkle trees are commonly used in distributed systems and cryptocurrencies to ensure data consistency and integrity without transmitting the entire dataset.


It is a crucial concept in distributed systems where nodes must select a leader to coordinate and manage the distributed activities. Algorithms like Paxos and Raft are often employed for leader election, ensuring that one node takes charge while others follow. This leadership structure simplifies decision-making, improves coordination, and enhances the efficiency of distributed systems.


MongoDB: The most popular NoSQL database, is an open-source document-oriented database. The term ‘NoSQL’ means ‘non-relational’. It means that MongoDB isn’t based on the table-like relational database structure but provides an altogether different mechanism for storage and retrieval of data. This format of storage is called BSON
There are so many databases are available and picking up one database over another is a complicated decision. Well, there is no real formula you can follow but there are a few things you should think about. irstly set aside the idea that you are going to find the one true database that is better than everything else. Now ask a few important questions related to your project:
Note: Also check SQL vs NoSQL Database

A database schema is a logical representation of data that shows how the data in a database should be stored logically. It shows how the data is organized and the relationship between the tables. Database schema contains table, field, views and relation between different keys like primary key, foreign key.
Data is stored in the form of files which is unstructured in nature which makes accessing the data difficult. Thus to resolve the issue the data are organized in structured way with the help of database schema.
Database Queries: Queries are used to retrieve and manipulate data from databases using SQL or other query languages.

In order to maintain consistency in a database, before and after the transaction, certain properties are followed. These are called ACID properties. 

Sharding
Sharding repre­sents a technique use­d to enhance the scalability and pe­rformance of database manageme­nt for handling large amounts of data. This approach involves fragmenting the extensive datase­t into smaller, self-contained se­gments known as shards. These shards are­ then allocated to separate­ servers or nodes, facilitating paralle­lism in data processing. As a result, query re­sponse times are improve­d, high traffic loads can be accommodated, and bottlene­cks are mitigated.
Partitioning
Partitioning is an optimization technique­ in databases where a single­ table is divided into smaller se­gments called partitions. These­ partitions hold subsets of the table’s data base­d on specific criteria like value­ ranges or categories. This strate­gy enhances query pe­rformance by reducing the amount of scanne­d data, resulting in faster retrie­val times. Furthermore, partitioning simplifie­s maintenance tasks such as backup and indexing since­ they can be focused on individual partitions.
Indexing improves database performance by minimizing the number of disc visits required to fulfill a query. It is a data structure technique used to locate and quickly access data in databases. Several database fields are used to generate indexes. The main key or candidate key of the table is duplicated in the first column, which is the Search key. 
To speed up data retrieval, the values are also kept in sorted order. It should be highlighted that sorting the data is not required. The second column is the Data Reference or Pointer which contains a set of pointers holding the address of the disk block where that particular key value can be found.
LLD, as the name suggests, stands for low-level design. It is a component-level design process that follows step by step refinement process. The input to LLD is HLD. 
LLD describes class diagrams with the help of methods and relations between classes and program specs. It describes the modules so that the programmer can directly code the program from the document. It provides us with the structure and behavior of class as different entities have different character sets. From this design, it is easy for a developer to write down logic and henceforth the actual code for it. 
Distributed System is a collection of autonomous computer systems that are physically separated but are connected by a centralized computer network that is equipped with distributed system software. The autonomous computers will communicate among each system by sharing resources and files and performing the tasks assigned to them.

MapReduce is a programming model used for efficient processing in parallel over large data-sets in a distributed manner. The data is first split and then combined to produce the final result. The libraries for MapReduce is written in so many programming languages with various different-different optimizations. 
The purpose of MapReduce in Hadoop is to Map each of the jobs and then it will reduce it to equivalent tasks for providing less overhead over the cluster network and to reduce the processing power. The MapReduce task is mainly divided into two phases Map Phase and Reduce Phase. 
12.2.1 Stateless Systems
In a stateless system, each request from a client to a server is independent and self-contained. The server does not retain any information about the client’s previous requests or state. Every request is treated as new, and the server doesn’t store any data or context about the client between requests. 
This design philosophy simplifies scalability since any server can handle any request from the client. Stateless systems are often more fault-tolerant and easier to scale horizontally because each request is isolated, and there are no dependencies on previous interactions.
12.2.2 Stateful systems
In a stateful system, the server keeps track of the state or context of the client across multiple requests. The server retains information about the client’s interactions, allowing for a continuous and personalized experience. This is particularly useful for applications that involve sessions, user authentication, or complex workflows where the server needs to remember past actions or maintain a certain state. Stateful systems offer advantages in terms of convenience and efficiency.


It is a consensus algorithm designed to manage a replicated log in a distributed system. Developed by Diego Ongaro and John Ousterhout, Raft aims to provide a straightforward yet efficient approach to distributed consensus. The primary use case for Raft is ensuring that a group of nodes (servers) can agree on a sequence of entries in a log, even if some nodes may fail or behave unpredictably.
Unified Modeling Language (UML) is a general purpose modelling language. The main aim of UML is to define a standard way to visualize the way a system has been designed. It is quite similar to blueprints used in other fields of engineering. UML is not a programming language, it is rather a visual language. We use UML diagrams to portray the behavior and structure of a system
Different Types of Diagrams:
Domain Name System (DNS) is a hostname for IP address translation service. DNS is a distributed database implemented in a hierarchy of name servers. It is an application layer protocol for message exchange between clients and servers. It is required for the functioning of the Internet.

A load balancer works as a “traffic cop” sitting in front of your server and routing client requests across all servers. It simply distributes the set of requested operations (database write requests, cache queries) effectively across multiple servers and ensures that no single server bears too many requests that lead to degrading the overall performance of the application. A load balancer can be a physical device or a virtualized instance running on specialized hardware or a software process. 
An N-tier architecture (also known as multi-tier or layered architecture) is a design approach for software applications that divides the entire application into a set of interconnected and logically separated layers or tiers. Each tier represents a specific functionality or set of related functionalities. 
This architecture enhances modularity, scalability, and maintainability by organizing the application into distinct layers, each with a specific role and responsibility. The “N” in N-tier represents the number of tiers or layers in the architecture, and it commonly refers to three-tier or four-tier architectures.
14.4.1 HTTP
It is the foundation of data communication on the World Wide Web. It is an application-layer protocol used for transmitting hypermedia documents, such as HTML. Developed primarily to facilitate communication between web browsers and servers, HTTP follows a client-server model, where the client initiates requests, and the server provides responses.
14.4.2 REST
REST is an architectural style for designing networked applications. It was introduced by Roy Fielding in his doctoral dissertation and emphasizes a stateless client-server communication model. RESTful systems adhere to a set of constraints to achieve simplicity, scalability, and uniformity.
It is a computing paradigm that involves the continuous processing of data streams in real-time. Unlike batch processing, where data is collected, stored, and processed in chunks, stream processing deals with data continuously as it is generated. This approach is crucial in scenarios where low-latency, real-time insights, and immediate actions on data are essential.
Caching is a system design concept that involves storing frequently accessed data in a location that is easily and quickly accessible. The purpose of caching is to improve the performance and efficiency of a system by reducing the amount of time it takes to access frequently accessed data.
Caching can be used in a variety of different systems, including web applications, databases, and operating systems. In each case, caching works by storing data that is frequently accessed in a location that is closer to the user or application. This can include storing data in memory or on a local hard drive.
Cache invalidation is a state where we push away the data from the cache memory when the data present is outdated so do we perform this operation of pushing back/flushing the cache otherwise this still data will result in inconsistency of data. When cached data gets stale or inaccurate, cache invalidation is the process of removing or updating it. The terms “purge,” “refresh,” and “ban” are often used in content delivery networks (CDNs), web proxies, and application caches as cache invalidation techniques.



Cache eviction is the process of removing data from a cache when the cache becomes full or when the data is no longer needed. There are several different types of cache eviction algorithms used by computer systems, including:
In this System Design Interview Bootcamp, ensuring the security of the systems is a top-notch priority. This article will deep into the aspects of why it is necessary to build secure systems and maintain them. With various threats like cyberattacks, Data Breaches, and other Vulnerabilities, it has become very important for system administrators to incorporate robust security measures into their systems. 
Some of the ways to ensure the security of your system are:
System design in machine learning is vital for scalability, performance, and efficiency. It ensures effective data management, model deployment, monitoring, and resource optimization, while also addressing security, privacy, and regulatory compliance. A well-designed system enables seamless integration, adaptability, cost control, and collaborative development, ultimately making machine learning solutions robust, reliable, and capable of real-world deployment.
Containerization is a lightweight form of virtualization that allows applications and their dependencies to be packaged and run consistently across different computing environments. Containers encapsulate an application, its runtime, libraries, and other dependencies, ensuring that it runs consistently regardless of the environment in which it is deployed. Docker, a widely used containerization platform, popularized this approach, but other containerization technologies exist, such as containerd and Podman.


Cloud computing is like renting tools or services over the internet instead of owning them on your own computer. Big companies, called cloud providers, take care of these services in their special buildings called data centers. This way, you don’t have to worry about fixing problems like you would with your own computer. It’s also cheaper, easier to use, and can grow or shrink based on what you need.
There are different companies that offer various services in the cloud, like storing your files, making sure things are secure, and managing who gets access to what. These services help you create flexible and efficient systems. The way you use these cloud services can be different, like using multiple clouds at once, mixing them with your own system, or just sticking to one. It’s like having different options for how you want to organize your stuff in the cloud
Follow these links for cracking the system design interviews:
Note: Also checkout latest System Design articles here: System-Design Archives

J


